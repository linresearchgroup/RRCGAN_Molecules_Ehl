{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564dafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc, rcParams\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,\n",
    "                          Concatenate, ReLU, Flatten, Dense, Embedding,\n",
    "                          BatchNormalization, Activation, SpatialDropout1D,\n",
    "                          Conv2D, MaxPooling2D, UpSampling2D, Lambda)\n",
    "from tensorflow.keras.models     import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses     import mse, binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.metrics import  mean_squared_error as mse_keras\n",
    "from tensorflow.keras.backend import argmax as argmax\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import one_hot\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "from tensorflow import random as randomtf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "from   matplotlib.lines import Line2D\n",
    "from   matplotlib.colors import ListedColormap\n",
    "import matplotlib.ticker as tk\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import seaborn as sns\n",
    "\n",
    "from chainer_chemistry.dataset.preprocessors import GGNNPreprocessor, construct_atomic_number_array\n",
    "preprocessor = GGNNPreprocessor()\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "from rdkit import Chem\n",
    "\n",
    "import ntpath\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "\"\"\" fix all the seeds,results are still slighthly different \"\"\"\n",
    "randomtf.set_seed(1)\n",
    "os.environ['PYTHONHASHSEED'] = '1'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "# import the shared libraries\n",
    "import sys\n",
    "sys.path.append(\"./../utils/\")\n",
    "from general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa193c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options)\n",
    "tf.compat.v1.set_random_seed(1234)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the font !!!!!!!!!!!!!!!!!!!!!\n",
    "# switch to Arial\n",
    "# if not working: delet ~/.catch/matplotlib\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "mpl.font_manager.FontManager()\n",
    "\n",
    "rc('font', weight='bold')\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plt.scatter([10, 55], [10, 55])\n",
    "ax.tick_params(axis='both', length=0, width=1.5, colors='black', grid_alpha=0, labelsize=20)\n",
    "plt.xlabel('!!!Ariaaaal', fontname='Arial', fontsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ff019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.keras.backend.clear_session()\n",
    "with open('./../data/trainingsets/train_regular_pubqc130K/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "tokenizer[0] = ' '\n",
    "with open('./../data/trainingsets/train_regular_pubqc130K/tokenizer_object.pickle', 'rb') as f:\n",
    "    tokenizer_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/trainingsets/highgap_outliergen_trans2/image.pickle', 'rb') as f:\n",
    "    X_smiles_gen, SMILES_gen, gen_train = pickle.load(f) \n",
    "    \n",
    "with open('./../data/trainingsets/highgap_outliergen_trans2/image_train.pickle', 'rb') as f:\n",
    "    X_smiles_train_gen, SMILES_train_gen, y_train_gen = pickle.load(f)\n",
    "    \n",
    "with open('./../data/trainingsets/highgap_outliergen_trans2/image_test.pickle', 'rb') as f:\n",
    "    X_smiles_val_gen, SMILES_val_gen, y_val_gen = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES_gen = np.array (SMILES_gen)\n",
    "gen_train = np.array (gen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smiles00 =  X_smiles_gen.copy()\n",
    "SMILES00 = SMILES_gen.copy()\n",
    "y_gantrain000 = gen_train.copy()\n",
    "print (y_gantrain000.shape)\n",
    "print (X_smiles00.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smiles0 = np.concatenate((X_smiles00, \n",
    "                            X_smiles00[y_gantrain000>10.2], \n",
    "                            X_smiles00[y_gantrain000>10.2], \n",
    "                            X_smiles00[y_gantrain000>10.2], \n",
    "                            X_smiles00[y_gantrain000>10.5], ), axis=0)\n",
    "SMILES0 = np.concatenate((SMILES00, \n",
    "                          SMILES00[y_gantrain000>10.2], \n",
    "                          SMILES00[y_gantrain000>10.2],\n",
    "                          SMILES00[y_gantrain000>10.2], \n",
    "                          SMILES00[y_gantrain000>10.5]), axis=0)\n",
    "y_gantrain00 = np.concatenate((y_gantrain000, \n",
    "                               y_gantrain000[y_gantrain000>10.2], \n",
    "                               y_gantrain000[y_gantrain000>10.2],\n",
    "                               y_gantrain000[y_gantrain000>10.2], \n",
    "                               y_gantrain000[y_gantrain000>10.5]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (SMILES0.shape)\n",
    "y_gantrain0 = y_gantrain00[y_gantrain00<=20]\n",
    "X_smiles = X_smiles0[y_gantrain00<=20]\n",
    "SMILES = SMILES0[y_gantrain00<=20]\n",
    "print (SMILES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6af013",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smiles_train00 = X_smiles_train_gen.copy() \n",
    "SMILES_train00 = SMILES_train_gen.copy() \n",
    "y_train000 = y_train_gen.copy() \n",
    "print (y_train000.shape)\n",
    "print (X_smiles_train00.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smiles_train0 = np.concatenate((X_smiles_train00, \n",
    "                                  X_smiles_train00[y_train000>10.2],\n",
    "                                  X_smiles_train00[y_train000>10.2],\n",
    "                                  X_smiles_train00[y_train000>10.2],\n",
    "                                  X_smiles_train00[y_train000>10.5],), axis=0)\n",
    "SMILES_train0 = np.concatenate((SMILES_train00, \n",
    "                                SMILES_train00[y_train000>10.2],\n",
    "                                SMILES_train00[y_train000>10.2],\n",
    "                                SMILES_train00[y_train000>10.2],\n",
    "                                SMILES_train00[y_train000>10.5],), axis=0)\n",
    "\n",
    "y_train00 = np.concatenate((y_train000, \n",
    "                                y_train000[y_train000>10.2],\n",
    "                                y_train000[y_train000>10.2],\n",
    "                                y_train000[y_train000>10.2],\n",
    "                                y_train000[y_train000>10.5],), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train0 = y_train00[y_train00<=20]\n",
    "X_smiles_train = X_smiles_train0[y_train00<=20]\n",
    "SMILES_train = SMILES_train0[y_train00<=20]\n",
    "SMILES_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smiles_val00 = X_smiles_val_gen.copy() \n",
    "SMILES_val00 = SMILES_val_gen.copy() \n",
    "y_val000 = y_val_gen.copy() \n",
    "print (y_val000.shape)\n",
    "print (X_smiles_val00.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smiles_val0 = np.concatenate((X_smiles_val00, \n",
    "                                  X_smiles_val00[y_val000>10.2],\n",
    "                                  X_smiles_val00[y_val000>10.2],\n",
    "                                  X_smiles_val00[y_val000>10.2],\n",
    "                                  X_smiles_val00[y_val000>10.5],), axis=0)\n",
    "SMILES_val0 = np.concatenate((SMILES_val00, \n",
    "                                SMILES_val00[y_val000>10.2],\n",
    "                                SMILES_val00[y_val000>10.2],\n",
    "                                SMILES_val00[y_val000>10.2],\n",
    "                                SMILES_val00[y_val000>10.5], ), axis=0)\n",
    "\n",
    "y_val00 = np.concatenate((y_val000, \n",
    "                                y_val000[y_val000>10.2],\n",
    "                                y_val000[y_val000>10.2],\n",
    "                                y_val000[y_val000>10.2],\n",
    "                                y_val000[y_val000>10.5],), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27800ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val0 = y_val00[y_val00<=20]\n",
    "X_smiles_val = X_smiles_val0[y_val00<=20]\n",
    "SMILES_val = SMILES_val0[y_val00<=20]\n",
    "SMILES_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized between 0 and 11\n",
    "gap_min = 0\n",
    "gap_max = 15\n",
    "y_val   = (y_val0 -   gap_min) / (gap_max - gap_min)\n",
    "y_train = (y_train0 - gap_min) / (gap_max - gap_min)\n",
    "y_gantrain = (y_gantrain0 - gap_min) / (gap_max - gap_min)\n",
    "\n",
    "y_gantrain0\n",
    "print (np.max(y_val))\n",
    "print (np.max(y_train))\n",
    "print (np.min(y_val))\n",
    "print (np.min(y_train))\n",
    "print (np.min(y_gantrain))\n",
    "print (np.max(y_gantrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaed59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "ax.tick_params(axis='both', length=4, width=2, colors='black', grid_alpha=0, labelsize=15)\n",
    "[i.set_linewidth(2) for i in ax.spines.values()]\n",
    "plt.xlabel('HOMO-LUMO gap (eV)', fontname='Arial', fontweight = 'bold', fontsize=15)\n",
    "plt.ylabel('% of samples', fontname='Arial', fontweight = 'bold', fontsize=15)\n",
    "\n",
    "sns.histplot (y_train0, color='red', label='train', stat='percent', kde=True, bins=50)\n",
    "sns.histplot (y_val0, color='blue', label='test', stat='percent', kde=True, bins=50, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('gap_val_train.jpeg', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" models definition and extracting pretrained encoder and decoder \"\"\"\n",
    "encoder = load_model('./../data/nns/keep/encoder_trans2.h5')\n",
    "decoder = load_model('./../data/nns/keep/decoder_trans2.h5')\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Filters = [256, 128, 64]\n",
    "        self.genFilters = [128, 128, 128]\n",
    "        self.upFilters = [(2, 2), (2, 2), (2, 2)]\n",
    "        \n",
    "config = Config()\n",
    "\n",
    "## Generator \n",
    "z = Input(shape = (128, ))\n",
    "y = Input(shape = (1, ))\n",
    "\n",
    "h = Concatenate(axis = 1)([z, y])\n",
    "h = Dense(1 * 1 * 128)(h)\n",
    "R1 = Reshape([1, 1, 128])(h)\n",
    "R2 = Reshape([1, 1, 128])(h)\n",
    "\n",
    "for i in range(3):\n",
    "    R1 = UpSampling2D(size = config.upFilters[i])(R1)\n",
    "    C1 = Conv2D(filters = config.genFilters[i], \n",
    "               kernel_size = 2, \n",
    "               strides = 1, \n",
    "               padding = 'same')(R1)\n",
    "    B1 = BatchNormalization()(C1)\n",
    "    R1 = LeakyReLU(alpha=0.2)(B1)\n",
    "\n",
    "for i in range(3):\n",
    "    R2 = UpSampling2D(size = config.upFilters[i])(R2)\n",
    "    C2 = Conv2D(filters = config.genFilters[i], \n",
    "               kernel_size = 2, \n",
    "               strides = 1, \n",
    "               padding = 'same')(R2)\n",
    "    B2 = BatchNormalization()(C2)\n",
    "    R2 = LeakyReLU(alpha=0.2)(B2)\n",
    "    \n",
    "R1 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh')(R1)\n",
    "R2 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh')(R2)\n",
    "\n",
    "generator0 = Model([z, y], [R1, R2])\n",
    "print (generator0.summary())\n",
    "\n",
    "## Discriminator \n",
    "inp1 = Input(shape = [6, 6, 1])\n",
    "inp2 = Input(shape = [6, 6, 1])\n",
    "\n",
    "X1 = Concatenate()([inp1, inp2])\n",
    "X = Flatten()(X1)\n",
    "y2 = Concatenate(axis = 1)([X, y])\n",
    "for i in range(3):\n",
    "\t\ty2 = Dense(64, activation = 'relu')(y2)\n",
    "\t\ty2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "\t\ty2 = Dropout(0.2)(y2)\n",
    "\n",
    "O_dis = Dense(1, activation = 'sigmoid')(y2)\n",
    "\n",
    "\n",
    "discriminator0 = Model([inp1, inp2, y], O_dis)\n",
    "discriminator0.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 5e-5, beta_1 = 0.5))\n",
    "print (discriminator0.summary()) \n",
    "\n",
    "## Regressor\n",
    "inp1 = Input(shape = [6, 6, 1])\n",
    "inp2 = Input(shape = [6, 6, 1])\n",
    "\n",
    "yr = Concatenate()([inp1, inp2])\n",
    "\n",
    "tower0 = Conv2D(64, 1, padding = 'same')(yr)\n",
    "tower1 = Conv2D(64, 1, padding = 'same')(yr)\n",
    "tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "tower2 = Conv2D(32, 1, padding = 'same')(yr)\n",
    "tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "tower3 = MaxPooling2D(3, 1, padding = 'same')(yr)\n",
    "tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "h = ReLU()(h)\n",
    "h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "\n",
    "for i in range(6):\n",
    "    tower0 = Conv2D(64, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "    tower2 = Conv2D(32, 1, padding = 'same')(h)\n",
    "    tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "    tower3 = MaxPooling2D(3, 1, padding = 'same')(h)\n",
    "    tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "    h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "    h = ReLU()(h)\n",
    "    if i % 2 == 0 and i != 0:\n",
    "        h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "h = BatchNormalization()(h)\n",
    "\n",
    "yr = Flatten()(h)\n",
    "o = Dropout(0.2)(yr)\n",
    "o = Dense(128)(o)\n",
    "\n",
    "o_reg = Dropout(0.2)(o)\n",
    "o_reg = Dense(1, activation = 'sigmoid')(o_reg)\n",
    "\n",
    "regressor0 = Model([inp1, inp2], o_reg)\n",
    "regressor_top0 = Model([inp1, inp2], o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator0 = load_model('./../data/nns/keep/discriminator_trans.h5')\n",
    "generator0 = load_model('./../data/nns/keep/generator_trans.h5')\n",
    "\n",
    "discriminator0.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Regressor \n",
    "# latent vectors from trained Encoder, \n",
    "# last output of Encoder is concat. (O1, O2)\n",
    "train_atoms_embedding, train_bonds_embedding, _ = encoder.predict([X_smiles_train], verbose=0)\n",
    "atoms_embedding, bonds_embedding, _ = encoder.predict([X_smiles_train], verbose=0)\n",
    "atoms_val, bonds_val, _ = encoder.predict([X_smiles_val], verbose=0)\n",
    "\n",
    "# No training if the trained model is saved. \n",
    "try:\n",
    "    regressor0 = load_model('./../data/nns/keep/regressor_trans.h5')\n",
    "    regressor_top0 = load_model('./../data/nns/keep/regressor_top_trans.h5')\n",
    "    regressor0.compile(loss = 'mse', optimizer = Adam(1e-8))\n",
    "    print (\".h5 was read\")\n",
    "except:\n",
    "    print (\"no .h5 available\")\n",
    "    regressor0.compile(loss = 'mse', optimizer = Adam(1e-8))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e952fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator0.summary(line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator0.trainable = False\n",
    "generator0.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator0.summary(line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator0.summary(line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54561de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_top0.trainable = False\n",
    "regressor0.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator0.layers.pop()\n",
    "generator0.layers.pop()\n",
    "\n",
    "discriminator0.layers.pop()\n",
    "discriminator0.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded94cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor0.layers.pop()\n",
    "regressor0.layers.pop()\n",
    "regressor0.layers.pop()\n",
    "regressor0.layers.pop()\n",
    "regressor0.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor0.summary(line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f510331",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dense_generator1 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh', name='mydense1')(generator0.layers[-3].output)\n",
    "\n",
    "new_dense_generator2 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh', name='mydense2')(generator0.layers[-4].output)\n",
    "\n",
    "inp_gen = generator0.input\n",
    "generator = Model([inp_gen[0], inp_gen[1]], [new_dense_generator1, new_dense_generator2])\n",
    "print (generator.summary(line_length=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_dense = Dense(1, activation = 'sigmoid', name='mydense')(discriminator0.layers[-2].output)\n",
    "inp_dist = discriminator0.input\n",
    "discriminator = Model([inp_dist[0], inp_dist[1], inp_dist[2]], dis_dense, name='discriminator')\n",
    "print (discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer1 = Dense(128, name='mydense1')(regressor0.layers[-4].output)\n",
    "new_layer2 = Dropout(0.2, name='mydrop')(new_layer1)\n",
    "new_layer3 = Dense(1, activation = 'sigmoid', name='mydense2')(new_layer2)\n",
    "inp = regressor0.input\n",
    "regressor = Model(inp, new_layer3, name='regressor')\n",
    "regressor.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = load_model('./../data/nns/keep/regressor_trans2.h5')\n",
    "regressor.compile(loss = 'mse', optimizer = Adam(1e-6))\n",
    "regressor0.trainable=False\n",
    "regressor.summary(line_length=150)\n",
    "\n",
    "history = regressor.fit([atoms_embedding, bonds_embedding], \n",
    "              y_train,\n",
    "              validation_data = ([atoms_val,\n",
    "                                  bonds_val],\n",
    "                                 y_val),\n",
    "              batch_size=8,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "    \n",
    "# Validating the regressor\n",
    "# Train\n",
    "pred_train = regressor.predict([atoms_embedding, bonds_embedding])\n",
    "pred_train0 = pred_train*(gap_max-gap_min)+gap_min\n",
    "print('Current R2 on Regressor for train data: {}'.format(r2_score(y_train0, pred_train0.reshape([-1]))))\n",
    "mse_train = mean_squared_error(y_train0, pred_train0.reshape([-1]))\n",
    "mae_train = mean_absolute_error(y_train0, pred_train0.reshape([-1]))\n",
    "\n",
    "#print ('prediction on train: ', pred_train)\n",
    "#print ('True train: ', y_train)\n",
    "\n",
    "# Test\n",
    "pred = regressor.predict([atoms_val, bonds_val])\n",
    "pred0 = pred*(gap_max-gap_min) + gap_min\n",
    "print('Current R2 on Regressor for test data: {}'.format(r2_score(y_val0, pred0.reshape([-1]))))\n",
    "mse_val = mean_squared_error (y_val0, pred0.reshape([-1]))\n",
    "mae_val = mean_absolute_error (y_val0, pred0.reshape([-1]))\n",
    "\n",
    "#print (\"prediction on test: \", pred )\n",
    "#print (\"True test values: \", y_val)\n",
    "\n",
    "print ('Train MSE: {}, RMSE: {}, MAE: {}'.format (round(mse_train, 5), \n",
    "                                                  round(mse_train**0.5, 5), \n",
    "                                                  round(mae_train, 5)))\n",
    "print ('Test MSE: {}, RMSE: {}, MAE: {}'.format (round(mse_val, 5), \n",
    "                                                  round(mse_val**0.5, 5), \n",
    "                                                  round(mae_val, 5)))\n",
    "# Saving the currently trained models\n",
    "regressor.save('./../data/nns/regressor_trans2.h5')\n",
    "#regressor_top.save('./../data/nns/regressor_top_trans.h5')\n",
    "\n",
    "# save the losses \n",
    "with open ('regressor_loss_0_150.csv', 'w') as f:\n",
    "    for key in history.history.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,history.history[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb365f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressor = load_model('./../data/nns/keep/regressor.h5')\n",
    "with open('./../data/trainingsets/image.pickle', 'rb') as f:\n",
    "    X_smiles_original, _, y_original0 = pickle.load(f) \n",
    "# Training the Regressor \n",
    "# latent vectors from trained Encoder, \n",
    "# last output of Encoder is concat. (O1, O2)\n",
    "\n",
    "train_atoms_embedding_try, train_bonds_embedding_try, _ = encoder.predict([X_smiles_original], verbose=0)\n",
    "pred_original = regressor.predict([train_atoms_embedding_try, train_bonds_embedding_try])\n",
    "pred_original0 = pred_original*(gap_max-gap_min) + gap_min\n",
    "\n",
    "print('pearsonr on original data: {}'.format(r2_score(y_original0, pred_original0.reshape([-1]))))\n",
    "print('MAE on original data: {}'.format(mean_absolute_error(y_original0, pred_original0.reshape([-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615efb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "print('pearsonr on original data: {}'.format(pearsonr(y_original0, pred_original0.reshape([-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caefd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter (y_original0, pred_original0.reshape([-1]), s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.max(y_train0))\n",
    "print (np.max(pred_train0))\n",
    "print (np.max(y_val0))\n",
    "print (np.max(pred0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_pred_des = np.round (mean_absolute_error(pred0, y_val0), 4)\n",
    "print (\"MAE_pred_des\", MAE_pred_des)\n",
    "# Fractioned MAE, more normalized\n",
    "Fractioned_MAE_pred_des = 0\n",
    "for pred, true in zip(pred0, y_val0):\n",
    "        Fractioned_MAE_pred_des = Fractioned_MAE_pred_des +  abs(pred-true)/true\n",
    "Fractioned_MAE_pred_des = Fractioned_MAE_pred_des/(pred0.shape[0])\n",
    "print (\"MAEF_pred_des\", Fractioned_MAE_pred_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plt.plot(list(range(len(history.history['loss']))), \n",
    "         history.history['loss'], label='Training loss', linewidth=3,) \n",
    "plt.plot(list(range(len(history.history['val_loss']))), \n",
    "         history.history['val_loss'], label='Validation loss', linewidth=3,) \n",
    "\n",
    "ax.set_xlabel('Epochs', fontsize='20', fontname='Arial', fontweight='bold', labelpad=5)\n",
    "ax.set_ylabel('Loss', fontsize='20', fontname='Arial', fontweight='bold', labelpad=5)\n",
    "\n",
    "ax.tick_params(direction='out', length=5, width=3, colors='black', grid_alpha=1, labelsize='18')\n",
    "\n",
    "[i.set_linewidth(2) for i in ax.spines.values()]\n",
    "\n",
    "\n",
    "#plt.title ('per {}, rand {}'.format(per, rand))\n",
    "\n",
    "#plt.ylim(0, 12)\n",
    "#plt.xticks((1, 3, 5, 7, 9,  11));\n",
    "#plt.yticks((1, 3, 5, 7, 9,  11));\n",
    "plt.title('Regressor loss', fontsize=15, fontname='Arial', fontweight='bold', pad=10)\n",
    "plt.legend(fontsize=15) \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"R_loss_0_100.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360345c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plt.rcParams[\"legend.markerscale\"] = 1\n",
    "\n",
    "plt.scatter (y_train0, pred_train0, color='red', label='Train', alpha=0.6, s=10)\n",
    "plt.scatter ( y_val0, pred0, color='blue', label='Test', alpha=0.6, s=10)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xlabel('DFT gap (eV)', fontsize='20', fontname='Arial', fontweight='bold', labelpad=5)\n",
    "ax.set_ylabel('Pred. gap (eV)', fontsize='20', fontname='Arial', fontweight='bold', labelpad=5)\n",
    "\n",
    "ax.tick_params(direction='out', length=5, width=3, colors='black', \n",
    "               grid_alpha=1, labelsize='18')\n",
    "\n",
    "[i.set_linewidth(3) for i in ax.spines.values()]\n",
    "leg = plt.legend(title='Train: R$^2$={}, MAE={} \\nTest: R$^2$={}, MAE={}'.\\\n",
    "           format(round(r2_score(y_train0, pred_train0.reshape([-1])), 2), \n",
    "                  round (mae_train, 2),\n",
    "                  round (r2_score(y_val0, pred0.reshape([-1])), 2), \n",
    "                  round (mae_val, 2), ), framealpha=0, title_fontsize=10)\n",
    "leg._legend_box.align = \"left\"\n",
    "\n",
    "#plt.title ('per {}, rand {}'.format(per, rand))\n",
    "plt.xlim(7, 13)\n",
    "plt.ylim(7, 13)\n",
    "#plt.xticks((1, 3, 5, 7, 9,  11));\n",
    "#plt.yticks((1, 3, 5, 7, 9,  11));\n",
    "plt.plot([0, 12], [0, 12], '--k', )#color='black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('regressor_train_val.jpeg', dpi=300)\n",
    "plt.rcParams[\"legend.markerscale\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined model \n",
    "discriminator.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 5e-5, beta_1 = 0.5))\n",
    "def build_combined(z, y,\n",
    "                   regressor,\n",
    "                   discriminator,\n",
    "                   encoder,\n",
    "                   decoder):\n",
    "    discriminator0.trainable = False\n",
    "    regressor.trainable = False\n",
    "    encoder.trainable = False\n",
    "    decoder.trainable = False\n",
    "    \n",
    "    atoms_emb, bonds_emb = generator([z, y])\n",
    "    dec_embedding = Concatenate()([atoms_emb, bonds_emb])\n",
    "    \n",
    "    softmax_smiles, _ = decoder([dec_embedding])\n",
    "    argmax_smiles = argmax (softmax_smiles, axis=2)\n",
    "    argmax_smiles = Reshape([40])(argmax_smiles)\n",
    "    smiles = one_hot(argmax_smiles, depth=27)\n",
    "    smiles = Reshape([40, 27, 1])(smiles)\n",
    "    latent_encoder_atom, latent_encoder_bond, _ = encoder ([smiles])\n",
    "    \n",
    "    y_pred = regressor([latent_encoder_atom, latent_encoder_bond])\n",
    "    valid = discriminator([atoms_emb, bonds_emb, y])\n",
    "    #print ('valid from comb', valid)\n",
    "\n",
    "    combined = Model([z, y], [valid, y_pred])\n",
    "\n",
    "    combined.compile(loss = ['binary_crossentropy',\n",
    "                             'mse'], \n",
    "                     loss_weights = [0.01, 25.0], \n",
    "                     optimizer = Adam(5e-6, beta_1 = 0.5))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined = build_combined(z, y,\n",
    "                          regressor,\n",
    "                          discriminator,\n",
    "                          encoder,\n",
    "                          decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAN on desired range 9-12\n",
    "p= multiprocessing.Process()\n",
    "\"\"\" Training RCGAN \"\"\"\n",
    "# loading pretrained models\n",
    "#regressor = load_model    ('./../data/nns/regressor_trans.h5')\n",
    "#regressor_top = load_model('./../data/nns/regressor_top.h5')\n",
    "\n",
    "# unfrozen layers of G and D were already read.\n",
    "#generator = load_model    ('./../data/nns/keep/generator_trans2.h5')\n",
    "#discriminator = load_model ('./../data/nns/keep/discriminator_trans2.h5')\n",
    "#combined = load_model ('./../data/nns/combined.h5')\n",
    "\n",
    "regressor.trainable = False\n",
    "\n",
    "# SMILES related information\n",
    "max_gen_atoms = 9\n",
    "bond_max = 9\n",
    "MAX_NB_WORDS = 27\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "batches = y_train0.shape[0] // batch_size\n",
    "threshold = 0.1 # defining accurate samples\n",
    "reinforce_n = 50 # 5*reinforce_n = fake sampling\n",
    "reinforce_sample = 500 # how many samples generated for Reinforcement\n",
    "\n",
    "# variable for storing generated data\n",
    "G_Losses = []\n",
    "D_Losses = []\n",
    "R_Losses = []\n",
    "D_Losses_real = []\n",
    "D_Losses_fake = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    D_loss = []\n",
    "    G_loss = []\n",
    "    R_loss = []\n",
    "    D_loss_real = []\n",
    "    D_loss_fake = []\n",
    "    \n",
    "    for b in range(batches):\n",
    "        \n",
    "\n",
    "        regressor.trainable = False\n",
    "\n",
    "        idx = np.arange(b * batch_size, (b + 1) * batch_size)\n",
    "        # rearrange the samples \n",
    "        idx = np.random.choice(idx, batch_size, replace = False)\n",
    "        \n",
    "        x_smiles_train = X_smiles_train[idx] \n",
    "        batch_y = y_gantrain[idx]\n",
    "        \n",
    "        batch_z = np.random.normal(0, 1, size = (batch_size, 128))\n",
    "        \n",
    "        atoms_embedding, bonds_embedding, _ = encoder.predict([x_smiles_train], verbose=0)\n",
    "        dec_embedding = np.concatenate([atoms_embedding, bonds_embedding], axis = -1)\n",
    "        \n",
    "        gen_atoms_embedding, gen_bonds_embedding = generator.predict([batch_z, batch_y], verbose=0)\n",
    "        \n",
    "        gen_dec_embedding = np.concatenate([gen_atoms_embedding, gen_bonds_embedding], axis = -1)\n",
    "        softmax_smiles = decoder.predict(gen_dec_embedding, verbose=0)[0]\n",
    "        \n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2)\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "        SHAPE = list(smiles.shape) + [1]\n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles], verbose=0)\n",
    "        gen_pred = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0).reshape([-1])\n",
    "        \n",
    "        regressor.trainable = True\n",
    "        r_loss = regressor.train_on_batch([atoms_embedding, bonds_embedding], batch_y)\n",
    "        R_loss.append(r_loss)\n",
    "        regressor.trainable = False\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        discriminator0.trainable = False\n",
    "        # original was 3!\n",
    "        d = 3\n",
    "        #if b<100:\n",
    "        #    d=1\n",
    "        for _ in range(d):\n",
    "            d_loss_real = discriminator.train_on_batch([atoms_embedding, bonds_embedding, batch_y],\n",
    "                                                       [0.9 * np.ones((batch_size, 1))])\n",
    "            d_loss_fake = discriminator.train_on_batch([gen_atoms_embedding, gen_bonds_embedding, batch_y],\n",
    "                                                       [np.zeros((batch_size, 1))]) \n",
    "\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        D_loss.append(d_loss)\n",
    "        D_loss_real.append (d_loss_real)\n",
    "        D_loss_fake.append (d_loss_fake)\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "\n",
    "        regressor.trainable = False\n",
    "\n",
    "        #for _ in range(d):\n",
    "        g_loss = combined.train_on_batch([batch_z, batch_y], [0.9 * np.ones((batch_size, 1)), batch_y])\n",
    "        \n",
    "        G_loss.append(g_loss[0])\n",
    "    \n",
    "    D_Losses.append(np.mean(D_loss))\n",
    "    D_Losses_real.append(np.mean(D_loss_real))\n",
    "    D_Losses_fake.append(np.mean(D_loss_fake))\n",
    "    G_Losses.append(np.mean(G_loss))\n",
    "    R_Losses.append(np.mean(R_loss))\n",
    "    \n",
    "    print('====')\n",
    "    print('Current epoch: {}/{}'.format((e + 1), epochs))\n",
    "    print ('D Loss Real: {}'.format(np.mean(D_loss_real)))\n",
    "    print ('D Loss Fake: {}'.format(np.mean(D_loss_fake)))\n",
    "    print('D Loss: {}'.format(np.mean(D_loss)))\n",
    "    print('G Loss: {}'.format(np.mean(G_loss)))\n",
    "    print('R Loss: {}'.format(np.mean(R_loss)))\n",
    "    print('====')\n",
    "    print()\n",
    "\n",
    "    \n",
    "    # Reinforcement\n",
    "    gen_error = []\n",
    "    gen_smiles = []\n",
    "    gen_valid_smiles = []\n",
    "    gen_X_atoms = []\n",
    "    gen_X_bonds = []\n",
    "    predcv_AE_latent = []\n",
    "    embeddings = []\n",
    "    sample_ys = []\n",
    "    valid_smiles_index = []\n",
    "    for _ in range(reinforce_sample):\n",
    "        # train only on data between 9 and 12\n",
    "        sample_y = np.random.uniform(10, 15, size = [1, ])\n",
    "        sample_y = np.round(sample_y, 4)\n",
    "        sample_y = (sample_y - gap_min) / (gap_max - gap_min)\n",
    "        sample_ys.append(sample_y)\n",
    "\n",
    "        sample_z = np.random.normal(0, 1, size = (1, 128))\n",
    "\n",
    "        sample_atoms_embedding, sample_bonds_embedding = generator.predict([sample_z, sample_y], verbose=0)\n",
    "        embeddings.append((sample_atoms_embedding, sample_bonds_embedding))\n",
    "        \n",
    "        dec_embedding = np.concatenate([sample_atoms_embedding, sample_bonds_embedding], axis = -1)\n",
    "        softmax_smiles = decoder.predict(dec_embedding, verbose=0)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2).reshape([-1])\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "        SHAPE = [1] + list(smiles.shape) + [1]\n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        c_smiles = ''\n",
    "        for s in argmax_smiles:\n",
    "            c_smiles += tokenizer[s]\n",
    "        c_smiles = c_smiles.rstrip()\n",
    "        \n",
    "        gen_smiles.append(c_smiles)\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles], verbose=0)\n",
    "        reg_pred = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0)\n",
    "        \n",
    "        pred, desire = reg_pred[0][0], sample_y[0]\n",
    "        gen_error.append(round (np.abs((pred - desire) / desire), 5)) #!!!!!!!!!!!!! chenge it to desire\n",
    "\n",
    "        \n",
    "    gen_error = np.asarray(gen_error).reshape(-1, 1)\n",
    "    # two validity defined: \n",
    "\n",
    "    # without sanitizing: valid 0    \n",
    "    valid = 0\n",
    "    valid0 = 0\n",
    "    idx_ = []\n",
    "    idx0_ = []\n",
    "    preds_can = []\n",
    "    for iter_, smiles in enumerate(gen_smiles):\n",
    "        if ' ' in smiles[:-1]:\n",
    "            continue\n",
    "        m  = Chem.MolFromSmiles(smiles[:-1], sanitize=True)\n",
    "        m0 = Chem.MolFromSmiles(smiles[:-1], sanitize=False)\n",
    "        if m0 is not None:\n",
    "            valid0 += 1\n",
    "            idx0_.append(iter_)\n",
    "        if m is not None:\n",
    "            #if len(construct_atomic_number_array(m)) <= 60:\n",
    "            valid += 1\n",
    "            idx_.append(iter_)\n",
    "            #try:\n",
    "            gen_smiles [iter_] = Chem.MolToSmiles(m, canonical=True)\n",
    "            smiles_can = Chem.MolToSmiles(m, canonical=True)\n",
    "            smiles_can_dot = smiles_can + '.'\n",
    "            X_smiles0 = tokenizer_.texts_to_sequences([smiles_can_dot])\n",
    "            X_smiles1 = pad_sequences(X_smiles0, maxlen = 40, padding = 'post')\n",
    "            X_smiles2 = to_categorical(X_smiles1, num_classes=27)\n",
    "            latent_encoder_atom, latent_encoder_bond, _ = encoder.predict(X_smiles2, verbose=0)\n",
    "            pred_can = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0).reshape([-1])\n",
    "            #pred_can = pred_can*15\n",
    "            preds_can.append(pred_can[0])\n",
    "            print (Chem.MolToSmiles(m, canonical=True))\n",
    "            print (\"gap_des\", sample_ys[iter_])\n",
    "            print (\"error\", gen_error[iter_])\n",
    "            #except:\n",
    "                #pass\n",
    "    idx_ = np.asarray(idx_)\n",
    "\n",
    "    idx0_ = np.asarray(idx0_)\n",
    "    preds_can = np.array (preds_can).reshape(-1, 1)\n",
    "    #print('gen error before correction:', gen_error [idx_])\n",
    "    sample_ys_ = np.array (sample_ys)\n",
    "    gen_error [idx_] = [(abs(i-j)/j) for i,j in zip(preds_can, sample_ys_[idx_])] \n",
    "    #print('gen error after correction:', gen_error [idx_])\n",
    "    validity = [gen_smiles[jj] for jj in idx0_ ]\n",
    "    validity = pd.DataFrame(validity)\n",
    "    validity = validity.drop_duplicates()\n",
    "\n",
    "    validity_sanitize = [gen_smiles[jj] for jj in idx_ ]\n",
    "    validity_sanitize = pd.DataFrame(validity_sanitize)\n",
    "    validity_sanitize = validity_sanitize.drop_duplicates()\n",
    "\n",
    "    if (e + 1) % 100 == 0:\n",
    "        reinforce_n += 10\n",
    "\n",
    "    # invalid smiles:\n",
    "    fake_indices1 = np.setdiff1d(np.arange(reinforce_sample), np.asarray(idx_))\n",
    "    fake_indices2 = np.intersect1d(np.where(gen_error > threshold)[0], idx_)\n",
    "    fake_indices = np.concatenate ((fake_indices1, fake_indices2))\n",
    "    fake_indices = np.random.choice(fake_indices, reinforce_n * 5, replace = False)\n",
    "\n",
    "    real_indices_ = np.intersect1d(np.where(gen_error <= threshold)[0], idx_)\n",
    "    sample_size =  len(real_indices_)\n",
    "    real_indices = np.random.choice(real_indices_, sample_size, replace = False)\n",
    "    \n",
    "    # Activating Reinforcement \n",
    "    if e >= 5:\n",
    "        discriminator.trainable = True\n",
    "        discriminator0.trainable = False\n",
    "\n",
    "        regressor.trainable = False\n",
    "        for real_index in real_indices:\n",
    "            #real_latent = regressor_top.predict([embeddings[real_index][0], embeddings[real_index][1]])\n",
    "            _ = discriminator.train_on_batch([embeddings[real_index][0], \n",
    "                                              embeddings[real_index][1], \n",
    "                                              sample_ys[real_index]],\n",
    "                                             [1 * np.ones((1, 1))])\n",
    "\n",
    "        for fake_index in fake_indices:\n",
    "            #fake_latent = regressor_top.predict([embeddings[fake_index][0], embeddings[fake_index][1]])\n",
    "            _ = discriminator.train_on_batch([embeddings[fake_index][0], \n",
    "                                              embeddings[fake_index][1] , \n",
    "                                              sample_ys[fake_index]],\n",
    "                                             [np.zeros((1, 1))])\n",
    "        discriminator.trainable = False\n",
    "\n",
    "    # ==== #\n",
    "    try:\n",
    "        print('Currently valid SMILES (No chemical_beauty and sanitize off): {}'.format(valid0))\n",
    "        print('Currently valid SMILES Unique (No chemical_beauty and sanitize off): {}'.format(len(validity)))\n",
    "        print('Currently valid SMILES Sanitized: {}'.format(valid))\n",
    "        print('Currently valid Unique SMILES Sanitized: {}'.format(len(validity_sanitize)))\n",
    "        print('Currently satisfying SMILES: {}'.format(len(real_indices_)))\n",
    "        print('Currently unique satisfying generation: {}'.format(len(np.unique(np.array(gen_smiles)[real_indices_]))))\n",
    "        #print('Gen Sample is: {}, for {}'.format(c_smiles, sample_y))\n",
    "        #print('Predicted val: {}'.format(reg_pred))\n",
    "        print('====')\n",
    "        print()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if (e + 1) % 5 == 0:\n",
    "        plt.close()\n",
    "        fig, ax = plt.subplots(figsize = (12, 10))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "        plt.plot(G_Losses, color='blue')\n",
    "        plt.plot(D_Losses, color='red')\n",
    "        plt.xlabel('epochs', fontsize=35)\n",
    "        plt.ylabel('loss', fontsize=35)\n",
    "        mpl.rcParams['axes.linewidth'] = 2.5\n",
    "        #plt.plot(R_Losses)\n",
    "        plt.legend(['G Loss', 'D Loss'], fontsize=30)\n",
    "        plt.savefig(\"G_D_losses{}.png\".format (e+1))\n",
    "    \n",
    "\n",
    "    n_unique = len(np.unique(np.array(gen_smiles)[real_indices_]))\n",
    "    n_valid = valid\n",
    "    if valid > 450 and n_unique > 350:\n",
    "        print('Criteria has satisified, training has ended')\n",
    "        break\n",
    "\n",
    "    end = time.time()\n",
    "    print (\"time for current epoch: \", (end - start))\n",
    "\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "with open('GAN_loss.pickle', 'wb') as f:\n",
    "    pickle.dump((G_Losses, D_Losses, R_Losses), f)\n",
    "\n",
    "# Saving the currently trained models\n",
    "#regressor.save('regressor.h5')\n",
    "#regressor_top.save('regressor_top.h5')\n",
    "generator.save('./../data/nns/generator_trans2.h5')\n",
    "discriminator.save('./../data/nns/discriminator_trans2.h5')\n",
    "combined.save('./../data/nns/combined_trans2.h5')\n",
    "\n",
    "p.start()\n",
    "pred_can\n",
    "p.join()\n",
    "print ('Done')\n",
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e15d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#encoder = load_model('./../data/nns/encoder_trans.h5')\n",
    "#decoder = load_model('./../data/nns/decoder_trans.h5')\n",
    "#model = load_model('./../data/nns/ae_model_trans.h5')\n",
    "\n",
    "#regressor = load_model    ('./../data/nns/keep/regressor.h5')\n",
    "#regressor_top = load_model('./../data/nns/keep/regressor_top.h5')\n",
    "generator = load_model    ('./../data/nns/generator_trans2.h5')\n",
    "discriminator= load_model ('./../data/nns/discriminator_trans2.h5')\n",
    "\n",
    "pbar = ProgressBar()\n",
    "max = 0.3\n",
    "\n",
    "randS = []\n",
    "rsquaredS = []\n",
    "MAE_S = []\n",
    "less20RE_perS = []\n",
    "output_lenS = []\n",
    "mean_RE_S = []\n",
    "\n",
    "for rand in pbar(range(0, 1)):  \n",
    "    N = 700\n",
    "    n_sample = 250\n",
    "    gen_error = []\n",
    "    gen_smiles = []\n",
    "    sample_ys = []\n",
    "    preds = []\n",
    "  \n",
    "    predss_can = []\n",
    "    gen_atoms_embedding = []\n",
    "    gen_bonds_embedding = []\n",
    "\n",
    "    regressor.trainable = False\n",
    "    generator.trainable = False\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    np.random.seed(105)\n",
    "    sample_yss = np.random.uniform(8, 15, size=n_sample)\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    for hc in (pbar(sample_yss)):\n",
    "        try:\n",
    "            # get it back to original of s_min to s_max\n",
    "            #sample_y = np.random.uniform(0, 15, size=[1,])\n",
    "            sample_y = hc\n",
    "            #print (sample_y)\n",
    "            sample_y = np.round(sample_y, 4)\n",
    "            sample_y = sample_y * np.ones([N, ])\n",
    "            sample_y_ = (sample_y - gap_min) / (gap_max - gap_min)\n",
    "            \n",
    "            sample_z = np.random.normal(0, 1, size = (N, 128))\n",
    "            \n",
    "            regressor.trainable = False\n",
    "            encoder.trainable = False\n",
    "            decoder.trainable = False\n",
    "\n",
    "            sample_atoms_embedding, sample_bonds_embedding = generator.predict([sample_z, sample_y_], verbose=0)\n",
    "            dec_embedding = np.concatenate([sample_atoms_embedding, sample_bonds_embedding], axis = -1)\n",
    "\n",
    "            softmax_smiles = decoder.predict(dec_embedding, verbose=0)[0]\n",
    "            argmax_smiles = np.argmax(softmax_smiles, axis = 2)\n",
    "            #print (argmax_smiles)\n",
    "\n",
    "            #print ('shape argmax_smiles', argmax_smiles.shape)\n",
    "            smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "            \n",
    "            SHAPE = list(smiles.shape) + [1] \n",
    "            \n",
    "            #print ('shape line 767', SHAPE) \n",
    "            smiles = smiles.reshape(SHAPE)\n",
    "\n",
    "            latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles], verbose=0)\n",
    "            pred = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0).reshape([-1])\n",
    "            pred = pred * (gap_max - gap_min) + gap_min\n",
    "\n",
    "            gen_errors = np.abs((pred - sample_y) / sample_y).reshape([-1])\n",
    "\n",
    "\n",
    "            smiles = decoder.predict(dec_embedding, verbose=0)[0]\n",
    "            #print(smiles)\n",
    "            smiles = np.argmax(smiles, axis = 2).reshape(smiles.shape[0], 40)\n",
    "            \n",
    "\n",
    "            generated_smiles = []\n",
    "            \n",
    "            for S in smiles:\n",
    "                c_smiles = ''\n",
    "                for s in S:\n",
    "                    c_smiles += tokenizer[s]\n",
    "                c_smiles = c_smiles.rstrip()\n",
    "                #print (c_smiles)\n",
    "                generated_smiles.append(c_smiles)\n",
    "            generated_smiles = np.array(generated_smiles)\n",
    "            #generated_smiles = generated_smiles [accurate]\n",
    "            all_gen_smiles = []\n",
    "            idx = []\n",
    "            preds_can = []\n",
    "            for i, smiles in enumerate(generated_smiles):\n",
    "                all_gen_smiles.append(smiles[:-1])\n",
    "\n",
    "                if ' ' in smiles[:-1]:\n",
    "                    continue\n",
    "                #m = Chem.MolFromSmiles(smiles[:-1], sanitize=False)\n",
    "                m = Chem.MolFromSmiles(smiles[:-1], sanitize=True)\n",
    "                if m is not None:\n",
    "                    \n",
    "                    smiles_can = Chem.MolToSmiles(m, canonical=True)\n",
    "                    smiles_can_dot = smiles_can + '.'\n",
    "                    X_smiles0 = tokenizer_.texts_to_sequences([smiles_can_dot])\n",
    "                    X_smiles1 = pad_sequences(X_smiles0, maxlen = 40, padding = 'post')\n",
    "                    X_smiles2 = to_categorical(X_smiles1, num_classes=27)\n",
    "                    latent_encoder_atom, latent_encoder_bond, _ = encoder.predict(X_smiles2, verbose=0)\n",
    "                    pred_can = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0).reshape([-1])\n",
    "                    pred_can = pred_can*15\n",
    "                    # focus on samples with high gap\n",
    "                    #if pred_can >10:\n",
    "                    #print(pred_can[0])\n",
    "                    preds_can.append(pred_can[0])\n",
    "                    idx.append(i)\n",
    "\n",
    "\n",
    "            idx = np.array(idx)\n",
    "            all_gen_smiles = np.array(all_gen_smiles)\n",
    "            #print ('all gen smiels shape', all_gen_smiles.shape)\n",
    "            #print ('gen_errors shape', gen_errors.shape)\n",
    "            #print (idx)\n",
    "            gen_smiles.extend(list(all_gen_smiles[idx]))\n",
    "            gen_error.extend(list(gen_errors[idx]))\n",
    "            sample_ys.extend(list(sample_y[idx]))\n",
    "            gen_atoms_embedding.extend(sample_atoms_embedding[idx])\n",
    "            gen_bonds_embedding.extend(sample_bonds_embedding[idx])\n",
    "            preds.extend(list(pred[idx]))\n",
    "            predss_can.extend(list(preds_can))\n",
    "        except:\n",
    "            #print('Did not discover SMILES for HC: {}'.format(sample_y))\n",
    "            pass    \n",
    "\n",
    "    try:\n",
    "        output = {}\n",
    "\n",
    "        for i, s in enumerate (gen_smiles):\n",
    "            ss = Chem.MolToSmiles(Chem.MolFromSmiles(s, sanitize=True), canonical=True)\n",
    "            gen_smiles[i] = ss\n",
    "\n",
    "        output['SMILES'] = gen_smiles\n",
    "        output['des_gap'] = sample_ys\n",
    "        # More accurate for regressor to predict gap from canonical SMILES\n",
    "        output['pred_gap'] = predss_can\n",
    "        #output['Err_pred_des'] = gen_error\n",
    "        output['Err_pred_des'] = [abs(i- j)/i for i, j in zip(output['des_gap'], output['pred_gap'])]\n",
    "        output = pd.DataFrame(output)\n",
    "        output.reset_index(drop = True, inplace = True)\n",
    "        #if (sum (output['Err_pred_des']<= 0.2))/output.shape[0] > 0.0:\n",
    "        output.to_csv ('./../experiments/transfer2/trans2_training{}.csv'.format(rand), index=False)\n",
    "        #print ((sum (output['Err_pred_des']<= 0.2))/output.shape[0] )\n",
    "\n",
    "        ## Statistics  (# pred=True value, Des=prediction)\n",
    "        # total # of samples\n",
    "        N = len(predss_can)\n",
    "        print ('random seed', rand)\n",
    "\n",
    "        # Explained Variance R2 from sklearn.metrics.explained_variance_score\n",
    "        explained_variance_R2_pred_des = explained_variance_score(output['des_gap'], output['pred_gap'])\n",
    "        print (\"explained_varice_R2_pred_des\", explained_variance_R2_pred_des)\n",
    "        rsquared = np.round (r2_score (output['des_gap'], output['pred_gap']), 4)\n",
    "        print (\"r squared r**2\", rsquared)\n",
    "\n",
    "        # mean absolute error \n",
    "        MAE_pred_des = np.round (mean_absolute_error(output['pred_gap'], output['des_gap']), 4)\n",
    "        print (\"MAE_pred_des\", MAE_pred_des)\n",
    "        # Fractioned MAE, more normalized\n",
    "        Fractioned_MAE_pred_des = 0\n",
    "        for pred, des in zip(output['pred_gap'], output['des_gap']):\n",
    "            Fractioned_MAE_pred_des = Fractioned_MAE_pred_des +  abs(des-pred)/des\n",
    "        Fractioned_MAE_pred_des = Fractioned_MAE_pred_des/N\n",
    "        #print (\"Fractioned MAE_pred_des\", Fractioned_MAE_pred_des)\n",
    "\n",
    "        # root mean squared error (RMSE), sqrt(sklearn ouputs MSE)\n",
    "        RMSE_pred_des = mean_squared_error(output['pred_gap'], output['des_gap'])**0.5\n",
    "        #print (\"RMSE_pred_des\", RMSE_pred_des)\n",
    "\n",
    "        Fractioned_RMSE_pred_des = 0\n",
    "        for pred, des in zip(output['pred_gap'], output['des_gap']):\n",
    "            Fractioned_RMSE_pred_des = Fractioned_RMSE_pred_des + ((des-pred)/des)**2\n",
    "        Fractioned_RMSE_pred_des = (Fractioned_RMSE_pred_des/N)**0.5\n",
    "        #print (\"Fractioned_RMSE_pred_des\", Fractioned_RMSE_pred_des)\n",
    "\n",
    "        # do not drop duplicate\n",
    "        output2 = output.drop_duplicates(['SMILES'])\n",
    "        output2.reset_index(drop = True, inplace = True)\n",
    "        #output2.to_csv('./../experiments/regular/Initial_training_nodub.csv', index = False)\n",
    "        \"\"\"with open('gen_pickles.pickle', 'wb') as f:\n",
    "            pickle.dump(gen_unique_pickles, f)\n",
    "        \"\"\"\n",
    "        #print ('% < 20 RE NODUP', sum (output2['Err_pred_des'] < 0.2) / output2['Err_pred_des'].shape[0])\n",
    "        less20RE_per = np.round ((sum(output['Err_pred_des'] <= 0.2) / output['Err_pred_des'].shape[0]), 4)\n",
    "        print ('% < 20 RE', less20RE_per)\n",
    "        output_len = len(output)\n",
    "        explained_variance_R2_pred_des = explained_variance_score(output['des_gap'], output['pred_gap'])\n",
    "        #print (\"explained_varice_R2_pred_des\", explained_variance_R2_pred_des)\n",
    "        mean_RE = np.round (np.mean (output['Err_pred_des']), 4)\n",
    "        print ('RE mean', mean_RE)\n",
    "\n",
    "        randS.append(rand)\n",
    "        rsquaredS.append(rsquared)\n",
    "        MAE_S.append(MAE_pred_des)\n",
    "        less20RE_perS.append(less20RE_per)\n",
    "        mean_RE_S.append(mean_RE)\n",
    "        output_lenS.append(output_len)\n",
    "\n",
    "        if rsquared>max:\n",
    "            good_rand = rand\n",
    "            max = rsquared\n",
    "            best_r2 = rsquared\n",
    "            print ('best r2', best_r2)\n",
    "            print ('best random seed', good_rand)\n",
    "        \n",
    "        tf.compat.v1.keras.backend.clear_session()\n",
    "        # 8.78 = mean, 11.48 = max\n",
    "        print (rand)\n",
    "        print (np.mean(output['pred_gap']))\n",
    "        print (np.max(output['pred_gap']))\n",
    "    except:\n",
    "        pass\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "\n",
    "params = {}\n",
    "params ['rand'] = randS\n",
    "params ['r2'] = rsquaredS\n",
    "params ['MAE'] = MAE_S\n",
    "params ['less20RE_per'] = less20RE_perS\n",
    "params ['Average_RE'] = mean_RE_S\n",
    "params ['total_valid'] = output_lenS\n",
    "params = pd.DataFrame(params)\n",
    "params.reset_index(drop = True, inplace = True)\n",
    "params.to_csv ('./gen_params.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
