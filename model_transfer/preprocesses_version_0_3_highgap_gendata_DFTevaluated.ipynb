{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import ndarray\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# tensorflow related libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# loading SMILES data using Chainer Chemistry\n",
    "from chainer_chemistry.datasets.molnet import get_molnet_dataset\n",
    "from chainer_chemistry.datasets.numpy_tuple_dataset import NumpyTupleDataset\n",
    "from chainer_chemistry.dataset.preprocessors import GGNNPreprocessor, construct_atomic_number_array\n",
    "\n",
    "# import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = './../data/trainingsets/highgap_outliergen_trans1/'\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "  \n",
    "data_gen = pd.DataFrame({})\n",
    "# loop over the list of csv files\n",
    "for f in csv_files:\n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f)\n",
    "    data_gen = pd.concat((data_gen, df), axis=0)\n",
    "    # print the location and filename\n",
    "    print('Location:', f)\n",
    "    print('File Name:', f.split(\"\\\\\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all the SMILES to Canonical format using rdkit\n",
    "preprocessor = GGNNPreprocessor()\n",
    "#atom_num = construct_atomic_number_array()\n",
    "#data_gen = pd.read_csv('./outliers17.csv')\n",
    "data_gen0 = data_gen.copy()\n",
    "\n",
    "gen_smiles = []\n",
    "idx = []\n",
    "for i, smile in enumerate(data_gen['SMILES']):\n",
    "    try:\n",
    "        gen_smiles.append (Chem.MolToSmiles(Chem.MolFromSmiles(smile, sanitize=True), canonical=True))\n",
    "        idx.append(i)\n",
    "    except:\n",
    "        print (smile)\n",
    "        pass\n",
    "idx = np.array(idx)\n",
    "data_gen = data_gen.iloc[idx]\n",
    "data_gen = data_gen.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Test all gen smiles are canonical:', \n",
    "       sum(gen_smiles==data_gen['SMILES'])==data_gen.shape[0])\n",
    "print (gen_smiles[0])\n",
    "\n",
    "#data_gen = data_gen.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n",
    "gen_smiles = data_gen['SMILES']\n",
    "try:\n",
    "    DFT_gap = data_gen['DFT_gap']\n",
    "except:\n",
    "    DFT_gap = data_gen['HOMO-LUMO gap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glob to get all the csv files from previous generation\n",
    "# put the previous files in the same folder\n",
    "path = './../data/trainingsets/highgap_outliergen_trans1/'\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "  \n",
    "data_gen_previus = pd.DataFrame({})\n",
    "# loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f)\n",
    "    data_gen_previus = pd.concat((data_gen_previus, df), axis=0)\n",
    "    # print the location and filename\n",
    "    #print('Location:', f)\n",
    "    print('File Name:', f.split(\"\\\\\")[-1])\n",
    "\n",
    "data_gen = data_gen.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the repetitives from train\n",
    "#previous_rep = pd.merge(data_gen, data_gen_previus, on = 'SMILES', how = 'inner')\n",
    "#print (\"Same generated SMILES compared to pubqc: \\n{}\".format(previous_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rep_smiles = previous_rep['SMILES']\n",
    "for i in range(data_gen.shape[0]):\n",
    "    if (data_gen['SMILES'].loc[i] in list(rep_smiles)):\n",
    "        print (i)\n",
    "        data_gen = data_gen.drop(i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_gen = data_gen.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n",
    "\n",
    "gen_smiles = data_gen['SMILES']\n",
    "try:\n",
    "    DFT_gap = data_gen['DFT_gap']\n",
    "except:\n",
    "    DFT_gap = data_gen['pred_gap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data_gen.shape)\n",
    "print (len(gen_smiles))\n",
    "print (len(DFT_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/trainingsets/60000_train_regular_pubqc/tokenizer_object.pickle', 'rb') as f:\n",
    "    tokenizer_ = pickle.load(f)\n",
    "\n",
    "with open('./../data/trainingsets/60000_train_regular_pubqc/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smiles = []\n",
    "for smile in gen_smiles:\n",
    "    print (smile)\n",
    "    m  = Chem.MolFromSmiles(smile, sanitize=True)\n",
    "    smiles_can = Chem.MolToSmiles(m, canonical=True)\n",
    "    smiles_can_dot = smiles_can + '.'\n",
    "    X_smiles0 = tokenizer_.texts_to_sequences([smiles_can_dot])\n",
    "    X_smiles1 = pad_sequences(X_smiles0, maxlen = 40, padding = 'post')\n",
    "    X_smiles2 = to_categorical(X_smiles1, num_classes=27)\n",
    "    SHAPE = list(X_smiles2.shape[1:])+[1]\n",
    "    X_smiles2 = X_smiles2.reshape(SHAPE)\n",
    "    X_smiles.append(X_smiles2)\n",
    "\n",
    "X_smiles = np.array (X_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot (DFT_gap, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_accurate.to_csv('gen_new_noscreen_all_joback.csv', index = False)\n",
    "preprocessor = GGNNPreprocessor()\n",
    "\n",
    "with open('./../data/trainingsets/image.pickle', 'rb') as f:\n",
    "    X_smiles_pubqc, SMILES_pubqc, gap_pubqc = pickle.load(f)\n",
    "    \n",
    "with open('./../data/trainingsets/60000_train_regular_pubqc/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "tokenizer[0] = ' '\n",
    "\n",
    "with open('./../data/trainingsets/60000_train_regular_pubqc/tokenizer_object.pickle', 'rb') as f:\n",
    "    tokenizer_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as canonical SMILES to find duplicates\n",
    "# the gen smiles already converted to Canonical\n",
    "SMILES_pubqc_can = []\n",
    "for s in SMILES_pubqc:\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles (s[:-1])\n",
    "        ss = Chem.MolToSmiles(m)\n",
    "        SMILES_pubqc_can.append(ss)\n",
    "    except Exception as error:\n",
    "        print (error)\n",
    "#SMILES = SMILES.astype('str')\n",
    "print ('First SMILES in pubqc', SMILES_pubqc[0])\n",
    "print (np.array(SMILES_pubqc).shape)\n",
    "print (SMILES_pubqc.shape)\n",
    "SMILES_pubqc_can = np.array(SMILES_pubqc_can)\n",
    "data_pubqc = {}\n",
    "data_pubqc ['SMILES'] = SMILES_pubqc_can\n",
    "data_pubqc ['gap'] = gap_pubqc\n",
    "data_pubqc = pd.DataFrame(data_pubqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the repetitives from train\n",
    "database_samples_rep = pd.merge(data_gen, data_pubqc, on = 'SMILES', how = 'inner')\n",
    "print ( \"Same generated SMILES compared to pubqc: \\n{}\".format(database_samples_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_smiles = database_samples_rep['SMILES']\n",
    "rep_idx = []\n",
    "for i in range(gen_smiles.shape[0]):\n",
    "    if (gen_smiles[i] in list(rep_smiles)):\n",
    "        #print (i)\n",
    "        rep_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.setdiff1d(list(range(len(gen_smiles))), rep_idx)\n",
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if do not want to remove the replicates\n",
    "X_smiles_norep = X_smiles.copy()\n",
    "gen_smiles_norep = gen_smiles.copy()\n",
    "DFT_gap_norep = DFT_gap.copy()\n",
    "\n",
    "# if you want to remove the replicates\n",
    "X_smiles_norep = X_smiles[idx]\n",
    "gen_smiles_norep = gen_smiles[idx]\n",
    "DFT_gap_norep0 = DFT_gap[idx]\n",
    "DFT_gap_norep = [np.round(i, 2) for i in DFT_gap_norep0]\n",
    "DFT_gap_norep = np.array (DFT_gap_norep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_9p8_gap =  DFT_gap_norep0[DFT_gap_norep >=9.8]\n",
    "higher_9p8_SMILES =  gen_smiles_norep[DFT_gap_norep >=9.8]\n",
    "temp = pd.DataFrame({})\n",
    "temp['SMILES'] = higher_9p8_SMILES\n",
    "temp['DFT_gap'] = higher_9p8_gap\n",
    "temp.to_csv('./temp.csv', index=False)\n",
    "\n",
    "gen_smiles_norep = gen_smiles_norep.reset_index(drop=True)\n",
    "DFT_gap_norep = DFT_gap_norep.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_smiles0 = []\n",
    "for smile in gen_smiles_norep:\n",
    "    s_dot = smile + '.'\n",
    "    gen_smiles0.append(s_dot)\n",
    "gen_smiles0 = np.array(gen_smiles0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES_nodot = []\n",
    "for i in gen_smiles0:\n",
    "    smile = i[:-1]\n",
    "    SMILES_nodot.append(smile)\n",
    "SMILES_nodot = np.array (SMILES_nodot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsampling\n",
    "np.random.seed(420)\n",
    "idx = np.random.choice(len(DFT_gap_norep), int(len(DFT_gap_norep) * 0.8), replace = False)\n",
    "X_smiles_train, SMILES_train, y_train = (X_smiles_norep[idx], \n",
    "                                         gen_smiles0[idx], \n",
    "                                         DFT_gap_norep[idx])\n",
    "\n",
    "idx_test = np.setdiff1d(list(range(len(DFT_gap_norep))), idx)\n",
    "X_smiles_test, SMILES_test, y_test = (X_smiles_norep[idx_test], \n",
    "                                      gen_smiles0[idx_test], \n",
    "                                      DFT_gap_norep[idx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train, bins=50, stat='percent')\n",
    "sns.histplot(y_test, bins=50, stat='percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need DFT evaluation for creating the dataset.\n",
    "\n",
    "with open('./../data/trainingsets/highgap_outliergen_trans1/image.pickle', 'wb') as f:\n",
    "    pickle.dump((X_smiles_norep, gen_smiles0, DFT_gap_norep), f)\n",
    "    \n",
    "with open('./../data/trainingsets/highgap_outliergen_trans1/image_train.pickle', 'wb') as f:\n",
    "    pickle.dump((X_smiles_train, SMILES_train, y_train), f)\n",
    "    \n",
    "with open('./../data/trainingsets/highgap_outliergen_trans1/image_test.pickle', 'wb') as f:\n",
    "    pickle.dump((X_smiles_test, SMILES_test, y_test), f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
