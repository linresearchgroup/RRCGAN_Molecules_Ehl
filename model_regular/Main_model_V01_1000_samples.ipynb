{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564dafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from matplotlib import rc, rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "from   matplotlib.lines import Line2D\n",
    "from   matplotlib.colors import ListedColormap\n",
    "import matplotlib.ticker as tk\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,\n",
    "                          Concatenate, ReLU, Flatten, Dense, Embedding,\n",
    "                          BatchNormalization, Activation, SpatialDropout1D,\n",
    "                          Conv2D, MaxPooling2D, UpSampling2D, Lambda)\n",
    "from tensorflow.keras.models     import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses     import mse, binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.metrics import  mean_squared_error as mse_keras\n",
    "from tensorflow.keras.backend import argmax as argmax\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import one_hot\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "from tensorflow import random as randomtf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import seaborn as sns\n",
    "\n",
    "from chainer_chemistry.dataset.preprocessors import GGNNPreprocessor, construct_atomic_number_array\n",
    "preprocessor = GGNNPreprocessor()\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "from rdkit import Chem\n",
    "\n",
    "import ntpath\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# import general file from utils as shared packages\n",
    "import sys\n",
    "sys.path.append(\"./../utils/\")\n",
    "from general import *\n",
    "\n",
    "randomtf.set_seed(10)\n",
    "os.environ['PYTHONHASHSEED'] = '10'\n",
    "np.random.seed(420)\n",
    "random.seed(123450)\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa193c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory control\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n",
    "                                        inter_op_parallelism_threads=1, gpu_options=gpu_options)\n",
    "tf.compat.v1.set_random_seed(1234)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the font !!!!!!!!!!!!!!!!!!!!!\n",
    "# switch to Arial\n",
    "# if not working in Linux delet ~/.catch/matplotlib\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "mpl.font_manager.FontManager()\n",
    "\n",
    "rc('font', weight='bold')\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plt.scatter([10, 55], [10, 55])\n",
    "ax.tick_params(axis='both', length=0, width=1.5, colors='black', grid_alpha=0, labelsize=20)\n",
    "plt.xlabel('!!!Ariaaaal', fontname='Arial', fontsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" reading and preprocessing data\"\"\"\n",
    "with open('./../data/trainingsets/train_regular_pubqc130K/image_train.pickle', 'rb') as f:\n",
    "    X_smiles_train0, SMILES_train0, y_train00 = pickle.load(f)\n",
    "    \n",
    "with open('./../data/trainingsets/train_regular_pubqc130K/image_test.pickle', 'rb') as f:\n",
    "    X_smiles_val0, SMILES_val0, y_val00 = pickle.load(f)\n",
    "\n",
    "with open('./../data/trainingsets/train_regular_pubqc130K/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "tokenizer[0] = ' '\n",
    "\n",
    "with open('./../data/trainingsets/train_regular_pubqc130K/tokenizer_object.pickle', 'rb') as f:\n",
    "    tokenizer_ = pickle.load(f)\n",
    "    \n",
    "print (X_smiles_train0.shape)\n",
    "print (X_smiles_val0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of reduceing the number of samples\n",
    "idx = np.random.choice(len(y_train00), int(len(y_train00) * 1), replace = False)\n",
    "X_smiles_train, SMILES_train, y_train0 = (X_smiles_train0[idx], SMILES_train0[idx], y_train00[idx])\n",
    "idx = np.random.choice(len(y_val00), int(len(y_val00) * 1), replace = False)\n",
    "X_smiles_val, SMILES_val, y_val0 = (X_smiles_val0[idx], SMILES_val0[idx], y_val00[idx])\n",
    "\n",
    "print (X_smiles_train.shape)\n",
    "print (X_smiles_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (min(y_train00))\n",
    "print (max(y_train00))\n",
    "print (min(y_val0))\n",
    "print (max(y_val0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized between 0 and 11\n",
    "gap_min = 0\n",
    "gap_max = 11\n",
    "\n",
    "y_val = NormalizeData(y_val0, min_data=gap_min, max_data=gap_max)\n",
    "y_train = NormalizeData(y_train0, min_data=gap_min, max_data=gap_max)\n",
    "\n",
    "print (max(y_val))\n",
    "print (max(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" models definition and extracting pretrained encoder and decoder \"\"\"\n",
    "# encoder and decoder should be trained using another file named embedding.\n",
    "encoder = load_model('./../data/nns/keep/encoder.h5')\n",
    "decoder = load_model('./../data/nns/keep/decoder.h5')\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Filters = [256, 128, 64]\n",
    "        self.genFilters = [128, 128, 128]\n",
    "        self.upFilters = [(2, 2), (2, 2), (2, 2)]\n",
    "        \n",
    "config = Config()\n",
    "\n",
    "## Generator \n",
    "z = Input(shape = (128, ))\n",
    "y = Input(shape = (1, ))\n",
    "\n",
    "h = Concatenate(axis = 1)([z, y])\n",
    "h = Dense(1 * 1 * 128)(h)\n",
    "R1 = Reshape([1, 1, 128])(h)\n",
    "R2 = Reshape([1, 1, 128])(h)\n",
    "\n",
    "for i in range(3):\n",
    "    R1 = UpSampling2D(size = config.upFilters[i])(R1)\n",
    "    C1 = Conv2D(filters = config.genFilters[i], \n",
    "               kernel_size = 2, \n",
    "               strides = 1, \n",
    "               padding = 'same')(R1)\n",
    "    B1 = BatchNormalization()(C1)\n",
    "    R1 = LeakyReLU(alpha=0.2)(B1)\n",
    "\n",
    "for i in range(3):\n",
    "    R2 = UpSampling2D(size = config.upFilters[i])(R2)\n",
    "    C2 = Conv2D(filters = config.genFilters[i], \n",
    "               kernel_size = 2, \n",
    "               strides = 1, \n",
    "               padding = 'same')(R2)\n",
    "    B2 = BatchNormalization()(C2)\n",
    "    R2 = LeakyReLU(alpha=0.2)(B2)\n",
    "    \n",
    "R1 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh')(R1)\n",
    "R2 = Conv2D(1,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = 'tanh')(R2)\n",
    "\n",
    "generator = Model([z, y], [R1, R2])\n",
    "\n",
    "## Discriminator \n",
    "inp1 = Input(shape = [6, 6, 1])\n",
    "inp2 = Input(shape = [6, 6, 1])\n",
    "\n",
    "X1 = Concatenate()([inp1, inp2])\n",
    "X = Flatten()(X1)\n",
    "y2 = Concatenate(axis = 1)([X, y])\n",
    "for i in range(3):\n",
    "\t\ty2 = Dense(64, activation = 'relu')(y2)\n",
    "\t\ty2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "\t\ty2 = Dropout(0.2)(y2)\n",
    "\n",
    "O_dis = Dense(1, activation = 'sigmoid')(y2)\n",
    "\n",
    "\n",
    "discriminator = Model([inp1, inp2, y], O_dis)\n",
    "discriminator.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 5e-5, beta_1 = 0.5))\n",
    "\n",
    "## Regressor\n",
    "inp1 = Input(shape = [6, 6, 1])\n",
    "inp2 = Input(shape = [6, 6, 1])\n",
    "\n",
    "yr = Concatenate()([inp1, inp2])\n",
    "\n",
    "tower0 = Conv2D(64, 1, padding = 'same')(yr)\n",
    "tower1 = Conv2D(64, 1, padding = 'same')(yr)\n",
    "tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "tower2 = Conv2D(32, 1, padding = 'same')(yr)\n",
    "tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "tower3 = MaxPooling2D(3, 1, padding = 'same')(yr)\n",
    "tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "h = ReLU()(h)\n",
    "h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "\n",
    "for i in range(6):\n",
    "    tower0 = Conv2D(64, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "    tower2 = Conv2D(32, 1, padding = 'same')(h)\n",
    "    tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "    tower3 = MaxPooling2D(3, 1, padding = 'same')(h)\n",
    "    tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "    h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "    h = ReLU()(h)\n",
    "    if i % 2 == 0 and i != 0:\n",
    "        h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "h = BatchNormalization()(h)\n",
    "\n",
    "yr = Flatten()(h)\n",
    "o = Dropout(0.2)(yr)\n",
    "o = Dense(128)(o)\n",
    "\n",
    "o_reg = Dropout(0.2)(o)\n",
    "o_reg = Dense(1, activation = 'sigmoid')(o_reg)\n",
    "\n",
    "regressor = Model([inp1, inp2], o_reg)\n",
    "regressor_top = Model([inp1, inp2], o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Regressor \n",
    "# pretrained encoder and decoder \n",
    "train_atoms_embedding, train_bonds_embedding, _ = encoder.predict([X_smiles_train], verbose=0)\n",
    "atoms_embedding, bonds_embedding, _ = encoder.predict([X_smiles_train], verbose=0)\n",
    "atoms_val, bonds_val, _ = encoder.predict([X_smiles_val], verbose=0)\n",
    "\n",
    "regressor.trainable=True\n",
    "regressor_top.trainable=True\n",
    "try:\n",
    "    regressor = load_model('./../data/nns/keep/regressor.h5')\n",
    "    regressor_top = load_model('./../data/nns/keep/regressor_top.h5')\n",
    "    regressor.compile(loss = 'mse', optimizer = Adam(5e-7))\n",
    "    print (\".h5 was read\")\n",
    "except:\n",
    "    print (\"no .h5 available\")\n",
    "    regressor.compile(loss = 'mse', optimizer = Adam(1e-6))\n",
    "    pass\n",
    "    \n",
    "history = regressor.fit([atoms_embedding, bonds_embedding], \n",
    "              y_train,\n",
    "              validation_data = ([atoms_val,\n",
    "                                  bonds_val],\n",
    "                                 y_val),\n",
    "              batch_size = 512,\n",
    "              epochs = 1,\n",
    "              verbose = 1)\n",
    "    \n",
    "# Validating the regressor\n",
    "# Train\n",
    "pred_train = regressor.predict([atoms_embedding, bonds_embedding])\n",
    "pred_train0 = pred_train*(gap_max-gap_min)+gap_min\n",
    "print('Current R2 on Regressor for train data: {}'.format(\n",
    "    r2_score(y_train0, pred_train0.reshape([-1]))))\n",
    "mse_train = mean_squared_error(y_train0, pred_train0.reshape([-1]))\n",
    "mae_train = mean_absolute_error(y_train0, pred_train0.reshape([-1]))\n",
    "\n",
    "# Test\n",
    "pred = regressor.predict([atoms_val, bonds_val])\n",
    "pred0 = pred*(gap_max-gap_min) + gap_min\n",
    "print('Current R2 on Regressor for test data: {}'.format(r2_score(y_val0, pred0.reshape([-1]))))\n",
    "mse_test = mean_squared_error (y_val0, pred0.reshape([-1]))\n",
    "mae_test = mean_absolute_error (y_val0, pred0.reshape([-1]))\n",
    "\n",
    "print ('Train MSE: {}, RMSE: {}, MAE: {}'.format (round(mse_train, 5), \n",
    "                                                  round(mse_train**0.5, 5), \n",
    "                                                  round(mae_train, 5)))\n",
    "print ('Test MSE: {}, RMSE: {}, MAE: {}'.format (round(mse_test, 5), \n",
    "                                                  round(mse_test**0.5, 5), \n",
    "                                                  round(mae_test, 5)))\n",
    "# Saving the currently trained models\n",
    "regressor.save('./../data/nns/regressor.h5')\n",
    "regressor_top.save('./../data/nns/regressor_top.h5')\n",
    "\n",
    "# min. and max. of prediction\n",
    "print (np.max(pred0))\n",
    "print (np.max(y_train0))\n",
    "print (np.max(pred_train0))\n",
    "print (np.max(y_val0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_pred_des = np.round (mean_absolute_error(pred0, y_val0), 4)\n",
    "print (\"MAE_pred_des\", MAE_pred_des)\n",
    "# Fractioned MAE, more normalized\n",
    "Fractioned_MAE_pred_des = 0\n",
    "for pred, true in zip(pred0, y_val0):\n",
    "        Fractioned_MAE_pred_des = Fractioned_MAE_pred_des +  abs(pred-true)/true\n",
    "Fractioned_MAE_pred_des = Fractioned_MAE_pred_des/(pred0.shape[0])\n",
    "print (\"MAEF_pred_des\", Fractioned_MAE_pred_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360345c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plt.rcParams[\"legend.markerscale\"] = 10\n",
    "plt.scatter (y_train0, pred_train0, color='red', label='Train', alpha=0.6, s=0.05)\n",
    "plt.scatter ( y_val0, pred0, color='blue', label='Test', alpha=0.6, s=0.05)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xlabel('DFT gap (eV)', fontsize='20', fontname='Arial', fontweight='bold', labelpad=5)\n",
    "ax.set_ylabel('Pred. gap (eV)', fontsize='20', fontname='Arial', fontweight='bold', labelpad=5)\n",
    "\n",
    "ax.tick_params(direction='out', length=5, width=3, colors='black', \n",
    "               grid_alpha=1, labelsize='18')\n",
    "\n",
    "[i.set_linewidth(3) for i in ax.spines.values()]\n",
    "leg = plt.legend(title='Train: R$^2$={}, MAE={} \\nTest: R$^2$={}, MAE={}'.\\\n",
    "           format(round(r2_score(y_train0, pred_train0.reshape([-1])), 2), \n",
    "                  round (mae_train, 2),\n",
    "                  round (r2_score(y_val0, pred0.reshape([-1])), 2), \n",
    "                  round (mae_test, 2), ), framealpha=0, title_fontsize=15)\n",
    "leg._legend_box.align = \"left\"\n",
    "\n",
    "plt.xlim(0, 12)\n",
    "plt.ylim(0, 12)\n",
    "plt.xticks((1, 3, 5, 7, 9,  11));\n",
    "plt.yticks((1, 3, 5, 7, 9,  11));\n",
    "plt.plot([0, 12], [0, 12], '--k', )#color='black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('regressor_train_test.jpeg', dpi=300)\n",
    "plt.rcParams[\"legend.markerscale\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined model, train the Generator inside this combined model\n",
    "def build_combined(z, y,\n",
    "                   regressor,\n",
    "                   regressor_top,\n",
    "                   discriminator,\n",
    "                   encoder,\n",
    "                   decoder):\n",
    "    discriminator.trainable = False\n",
    "    regressor_top.trainable = False\n",
    "    regressor.trainable = False\n",
    "    encoder.trainable = False\n",
    "    decoder.trainable = False\n",
    "    \n",
    "    atoms_emb, bonds_emb = generator([z, y])\n",
    "    dec_embedding = Concatenate()([atoms_emb, bonds_emb])\n",
    "    \n",
    "    softmax_smiles, _ = decoder([dec_embedding])\n",
    "    argmax_smiles = argmax (softmax_smiles, axis=2)\n",
    "    argmax_smiles = Reshape([40])(argmax_smiles)\n",
    "    smiles = one_hot(argmax_smiles, depth=27)\n",
    "    smiles = Reshape([40, 27, 1])(smiles)\n",
    "    latent_encoder_atom, latent_encoder_bond, _ = encoder ([smiles])\n",
    "    \n",
    "    y_pred = regressor([latent_encoder_atom, latent_encoder_bond])\n",
    "    valid = discriminator([atoms_emb, bonds_emb, y])\n",
    "    #print ('valid from comb', valid)\n",
    "\n",
    "    combined = Model([z, y], [valid, y_pred])\n",
    "\n",
    "    combined.compile(loss = ['binary_crossentropy',\n",
    "                             'mse'], \n",
    "                     loss_weights = [0.01, 25.0], \n",
    "                     optimizer = Adam(5e-6, beta_1 = 0.5))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined = build_combined(z, y,\n",
    "                          regressor,\n",
    "                          regressor_top,\n",
    "                          discriminator,\n",
    "                          encoder,\n",
    "                          decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = multiprocessing.Process()\n",
    "\"\"\" Training RCGAN \"\"\"\n",
    "# loading pretrained models\n",
    "regressor = load_model    ('./../data/nns/regressor.h5')\n",
    "regressor_top = load_model('./../data/nns/regressor_top.h5')\n",
    "#generator = load_model    ('./../data/nns/keep/generator.h5')\n",
    "#discriminator = load_model ('./../data/nns/keep/discriminator.h5')\n",
    "\n",
    "regressor_top.trainable = False\n",
    "regressor.trainable = False\n",
    "\n",
    "# SMILES related information\n",
    "max_gen_atoms = 9\n",
    "bond_max = 9\n",
    "MAX_NB_WORDS = 27\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "batches = y_train0.shape[0] // batch_size\n",
    "threshold = 0.2 # defining accurate samples\n",
    "reinforce_n = 50 # 5*reinforce_n = fake sampling\n",
    "reinforce_sample = 1000 # how many samples generated for Reinforcement\n",
    "\n",
    "# variable for storing generated data\n",
    "G_Losses = []\n",
    "D_Losses = []\n",
    "R_Losses = []\n",
    "D_Losses_real = []\n",
    "D_Losses_fake = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    D_loss = []\n",
    "    G_loss = []\n",
    "    R_loss = []\n",
    "    D_loss_real = []\n",
    "    D_loss_fake = []\n",
    "    \n",
    "    for b in range(batches):\n",
    "        \n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "\n",
    "        idx = np.arange(b * batch_size, (b + 1) * batch_size)\n",
    "        # rearrange the samples \n",
    "        idx = np.random.choice(idx, batch_size, replace = False)\n",
    "        \n",
    "        x_smiles_train = X_smiles_train[idx] \n",
    "        batch_y = y_train[idx]\n",
    "        \n",
    "        batch_z = np.random.normal(0, 1, size = (batch_size, 128))\n",
    "        \n",
    "        atoms_embedding, bonds_embedding, _ = encoder.predict([x_smiles_train], verbose=0)\n",
    "        dec_embedding = np.concatenate([atoms_embedding, bonds_embedding], axis = -1)\n",
    "        \n",
    "        gen_atoms_embedding, gen_bonds_embedding = generator.predict([batch_z, batch_y], verbose=0)\n",
    "        gen_dec_embedding = np.concatenate([gen_atoms_embedding, gen_bonds_embedding], axis = -1)\n",
    "\n",
    "        softmax_smiles = decoder.predict(gen_dec_embedding, verbose=0)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2)\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "        SHAPE = list(smiles.shape) + [1]\n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles], verbose=0)\n",
    "        gen_pred = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0).reshape([-1])\n",
    "        \n",
    "        regressor.trainable = True\n",
    "        r_loss = regressor.train_on_batch([atoms_embedding, bonds_embedding], batch_y)\n",
    "        R_loss.append(r_loss)\n",
    "        regressor.trainable = False\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        d = 3 # hyperparamter\n",
    "        for _ in range(d):\n",
    "            d_loss_real = discriminator.train_on_batch([atoms_embedding, bonds_embedding, batch_y],\n",
    "                                                       [0.9 * np.ones((batch_size, 1))])\n",
    "            d_loss_fake = discriminator.train_on_batch([gen_atoms_embedding, gen_bonds_embedding, batch_y],\n",
    "                                                       [np.zeros((batch_size, 1))]) \n",
    "\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        D_loss.append(d_loss)\n",
    "        D_loss_real.append (d_loss_real)\n",
    "        D_loss_fake.append (d_loss_fake)\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "\n",
    "        g_loss = combined.train_on_batch([batch_z, batch_y], [0.9 * np.ones((batch_size, 1)), batch_y])\n",
    "        G_loss.append(g_loss[0])\n",
    "    \n",
    "    D_Losses.append(np.mean(D_loss))\n",
    "    D_Losses_real.append(np.mean(D_loss_real))\n",
    "    D_Losses_fake.append(np.mean(D_loss_fake))\n",
    "    G_Losses.append(np.mean(G_loss))\n",
    "    R_Losses.append(np.mean(R_loss))\n",
    "    \n",
    "    print('====')\n",
    "    print('Current epoch: {}/{}'.format((e + 1), epochs))\n",
    "    print ('D Loss Real: {}'.format(np.mean(D_loss_real)))\n",
    "    print ('D Loss Fake: {}'.format(np.mean(D_loss_fake)))\n",
    "    print('D Loss: {}'.format(np.mean(D_loss)))\n",
    "    print('G Loss: {}'.format(np.mean(G_loss)))\n",
    "    print('R Loss: {}'.format(np.mean(R_loss)))\n",
    "    print('====')\n",
    "    print()\n",
    "\n",
    "    \n",
    "    # Reinforcement\n",
    "    gen_error = []\n",
    "    gen_smiles = []\n",
    "    gen_valid_smiles = []\n",
    "    gen_X_atoms = []\n",
    "    gen_X_bonds = []\n",
    "    predcv_AE_latent = []\n",
    "    embeddings = []\n",
    "    sample_ys = []\n",
    "    valid_smiles_index = []\n",
    "    for _ in range(reinforce_sample):\n",
    "        sample_y = np.random.uniform(gap_min, gap_max, size = [1, ])\n",
    "        sample_y = np.round(sample_y, 4)\n",
    "        sample_y = (sample_y - gap_min) / (gap_max - gap_min)\n",
    "        sample_ys.append(sample_y)\n",
    "\n",
    "        sample_z = np.random.normal(0, 1, size = (1, 128))\n",
    "\n",
    "        sample_atoms_embedding, sample_bonds_embedding = generator.predict([sample_z, sample_y], verbose=0)\n",
    "        embeddings.append((sample_atoms_embedding, sample_bonds_embedding))\n",
    "        \n",
    "        dec_embedding = np.concatenate([sample_atoms_embedding, sample_bonds_embedding], axis = -1)\n",
    "        softmax_smiles = decoder.predict(dec_embedding, verbose=0)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2).reshape([-1])\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "        SHAPE = [1] + list(smiles.shape) + [1]\n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        c_smiles = ''\n",
    "        for s in argmax_smiles:\n",
    "            c_smiles += tokenizer[s]\n",
    "        c_smiles = c_smiles.rstrip()\n",
    "        \n",
    "        gen_smiles.append(c_smiles)\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles], verbose=0)\n",
    "        reg_pred = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0)\n",
    "        \n",
    "        pred, desire = reg_pred[0][0], sample_y[0]\n",
    "        gen_error.append(round (np.abs((pred - desire) / desire), 5))\n",
    "\n",
    "        \n",
    "    gen_error = np.asarray(gen_error)\n",
    "    # two validity defined: \n",
    "    ## without sanitizing: valid0 , sanitized=valid\n",
    "    valid = 0\n",
    "    valid0 = 0\n",
    "    idx_ = []\n",
    "    idx0_ = []\n",
    "    for iter_, smiles in enumerate(gen_smiles):\n",
    "        if ' ' in smiles[:-1]:\n",
    "            continue\n",
    "        m  = Chem.MolFromSmiles(smiles[:-1], sanitize=True)\n",
    "        m0 = Chem.MolFromSmiles(smiles[:-1], sanitize=False)\n",
    "        if m0 is not None:\n",
    "            valid0 += 1\n",
    "            idx0_.append(iter_)\n",
    "        if m is not None:\n",
    "            valid += 1\n",
    "            idx_.append(iter_)\n",
    "            try:\n",
    "                gen_smiles [iter_] = Chem.MolToSmiles(m, canonical=True)\n",
    "                print (Chem.MolToSmiles(m, canonical=True))\n",
    "                print (\"gap_des\", sample_ys[iter_])\n",
    "                print (\"error\", gen_error[iter_])\n",
    "            except:\n",
    "                pass\n",
    "    idx_ = np.asarray(idx_)\n",
    "    idx0_ = np.asarray(idx0_)\n",
    "\n",
    "    validity = [gen_smiles[jj] for jj in idx0_ ]\n",
    "    validity = pd.DataFrame(validity)\n",
    "    validity = validity.drop_duplicates()\n",
    "\n",
    "    validity_sanitize = [gen_smiles[jj] for jj in idx_ ]\n",
    "    validity_sanitize = pd.DataFrame(validity_sanitize)\n",
    "    validity_sanitize = validity_sanitize.drop_duplicates()\n",
    "\n",
    "    if (e + 1) % 100 == 0:\n",
    "        reinforce_n += 10\n",
    "\n",
    "    # invalid smiles:\n",
    "    fake_indices1 = np.setdiff1d(np.arange(reinforce_sample), np.asarray(idx_))\n",
    "    fake_indices2 = np.intersect1d(np.where(gen_error > threshold)[0], idx_)\n",
    "    fake_indices = np.concatenate ((fake_indices1, fake_indices2))\n",
    "    fake_indices = np.random.choice(fake_indices, reinforce_n * 5, replace = False)\n",
    "\n",
    "    real_indices_ = np.intersect1d(np.where(gen_error <= threshold)[0], idx_)\n",
    "    sample_size =  len(real_indices_)\n",
    "    real_indices = np.random.choice(real_indices_, sample_size, replace = False)\n",
    "    \n",
    "    # Activating Reinforcement \n",
    "    # hyperparamter, how many initial training of GAN\n",
    "    if e >= 5:\n",
    "        discriminator.trainable = True\n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "        for real_index in real_indices:\n",
    "            _ = discriminator.train_on_batch([embeddings[real_index][0], \n",
    "                                              embeddings[real_index][1], \n",
    "                                              sample_ys[real_index]],\n",
    "                                             [1 * np.ones((1, 1))])\n",
    "\n",
    "        for fake_index in fake_indices:\n",
    "            _ = discriminator.train_on_batch([embeddings[fake_index][0], \n",
    "                                              embeddings[fake_index][1] , \n",
    "                                              sample_ys[fake_index]],\n",
    "                                             [np.zeros((1, 1))])\n",
    "        discriminator.trainable = False\n",
    "\n",
    "    # ==== #\n",
    "    try:\n",
    "        print('Currently valid SMILES (No chemical_beauty and sanitize off): {}'.format(valid0))\n",
    "        print('Currently valid SMILES Unique (No chemical_beauty and sanitize off): {}'.format(len(validity)))\n",
    "        print('Currently valid SMILES Sanitized: {}'.format(valid))\n",
    "        print('Currently valid Unique SMILES Sanitized: {}'.format(len(validity_sanitize)))\n",
    "        print('Currently satisfying SMILES: {}'.format(len(real_indices_)))\n",
    "        print('Currently unique satisfying generation: {}'.format(len(np.unique(np.array(gen_smiles)[real_indices_]))))\n",
    "        print('====')\n",
    "        print()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if (e + 1) % 5 == 0:\n",
    "        plt.close()\n",
    "        fig, ax = plt.subplots(figsize = (12, 10))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "        plt.plot(G_Losses, color='blue')\n",
    "        plt.plot(D_Losses, color='red')\n",
    "        plt.xlabel('epochs', fontsize=35)\n",
    "        plt.ylabel('loss', fontsize=35)\n",
    "        mpl.rcParams['axes.linewidth'] = 2.5\n",
    "        plt.legend(['G Loss', 'D Loss'], fontsize=30)\n",
    "        plt.savefig(\"G_D_losses{}.png\".format (e+1))\n",
    "    \n",
    "\n",
    "    n_unique = len(np.unique(np.array(gen_smiles)[real_indices_]))\n",
    "    n_valid = valid\n",
    "\n",
    "    end = time.time()\n",
    "    print (\"time for current epoch: \", (end - start))\n",
    "\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "with open('GAN_loss.pickle', 'wb') as f:\n",
    "    pickle.dump((G_Losses, D_Losses, R_Losses), f)\n",
    "\n",
    "# Saving the currently trained models\n",
    "generator.save('./../data/nns/generator.h5')\n",
    "discriminator.save('./../data/nns/discriminator.h5')\n",
    "combined.save('./../data/nns/combined.h5')\n",
    "\n",
    "p.start()\n",
    "p.join()\n",
    "tf.compat.v1.keras.backend.clear_session()\n",
    "print ('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e15d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = load_model('./../data/nns/keep/encoder.h5')\n",
    "decoder = load_model('./../data/nns/keep/decoder.h5')\n",
    "regressor = load_model    ('./../data/nns/keep/regressor.h5')\n",
    "generator = load_model    ('./../data/nns/keep/generator.h5')\n",
    "discriminator= load_model ('./../data/nns/keep/discriminator.h5')\n",
    "regressor_top.trainable = False\n",
    "regressor.trainable = False\n",
    "generator.trainable = False\n",
    "discriminator.trainable = False\n",
    "\n",
    "# how many repeat of samples\n",
    "N = 200\n",
    "n_sample = 150\n",
    "gen_error = []\n",
    "gen_smiles = []\n",
    "sample_ys = []\n",
    "preds = []\n",
    "predss_can = []\n",
    "gen_atoms_embedding = []\n",
    "gen_bonds_embedding = []\n",
    "\n",
    "np.random.seed(396)\n",
    "samples = np.random.uniform(1, 11, size=[n_sample, ])\n",
    "# progressbar\n",
    "pbar = ProgressBar()\n",
    "for sample in (pbar(samples)):\n",
    "    try:\n",
    "        sample_y = sample\n",
    "        #print (sample_y)\n",
    "        sample_y = np.round(sample_y, 4)\n",
    "        sample_y = sample_y * np.ones([N, ])\n",
    "        sample_y_ = (sample_y - gap_min) / (gap_max - gap_min)\n",
    "        sample_z = np.random.normal(0, 1, size = (N, 128))\n",
    "\n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "        encoder.trainable = False\n",
    "        decoder.trainable = False\n",
    "\n",
    "        sample_atoms_embedding, sample_bonds_embedding = generator.predict([sample_z, sample_y_], verbose=0)\n",
    "        dec_embedding = np.concatenate([sample_atoms_embedding, sample_bonds_embedding], axis = -1)\n",
    "        softmax_smiles = decoder.predict(dec_embedding, verbose=0)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2)\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "        SHAPE = list(smiles.shape) + [1] \n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles], verbose=0)\n",
    "        pred = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0).reshape([-1])\n",
    "        pred = pred * (gap_max - gap_min) + gap_min\n",
    "        gen_errors = np.abs((pred - sample_y) / sample_y).reshape([-1])\n",
    "        smiles = decoder.predict(dec_embedding, verbose=0)[0]\n",
    "        smiles = np.argmax(smiles, axis = 2).reshape(smiles.shape[0], 40)\n",
    "    \n",
    "        generated_smiles = []\n",
    "        for S in smiles:\n",
    "            c_smiles = ''\n",
    "            for s in S:\n",
    "                c_smiles += tokenizer[s]\n",
    "            c_smiles = c_smiles.rstrip()\n",
    "            generated_smiles.append(c_smiles)\n",
    "        generated_smiles = np.array(generated_smiles)\n",
    "\n",
    "        all_gen_smiles = []\n",
    "        idx = []\n",
    "        preds_can = []\n",
    "        for i, smiles in enumerate(generated_smiles):\n",
    "            all_gen_smiles.append(smiles[:-1])\n",
    "\n",
    "            if ' ' in smiles[:-1]:\n",
    "                continue\n",
    "            m = Chem.MolFromSmiles(smiles[:-1], sanitize=True)\n",
    "            if m is not None:\n",
    "                idx.append(i)\n",
    "                smiles_can = Chem.MolToSmiles(m, canonical=True)\n",
    "                smiles_can_dot = smiles_can + '.'\n",
    "                X_smiles0 = tokenizer_.texts_to_sequences([smiles_can_dot])\n",
    "                X_smiles1 = pad_sequences(X_smiles0, maxlen = 40, padding = 'post')\n",
    "                X_smiles2 = to_categorical(X_smiles1, num_classes=27)\n",
    "                latent_encoder_atom, latent_encoder_bond, _ = encoder.predict(X_smiles2, verbose=0)\n",
    "                pred_can = regressor.predict([latent_encoder_atom, latent_encoder_bond], verbose=0).reshape([-1])\n",
    "                pred_can = pred_can*gap_max\n",
    "                preds_can.append(pred_can[0])\n",
    "\n",
    "\n",
    "        idx = np.array(idx)\n",
    "        all_gen_smiles = np.array(all_gen_smiles)\n",
    "        gen_smiles.extend(list(all_gen_smiles[idx]))\n",
    "        gen_error.extend(list(gen_errors[idx]))\n",
    "        sample_ys.extend(list(sample_y[idx]))\n",
    "        gen_atoms_embedding.extend(sample_atoms_embedding[idx])\n",
    "        gen_bonds_embedding.extend(sample_bonds_embedding[idx])\n",
    "        preds.extend(list(pred[idx]))\n",
    "        predss_can.extend(list(preds_can))\n",
    "    except:\n",
    "        pass    \n",
    "\n",
    "\n",
    "output = {}\n",
    "\n",
    "for i, s in enumerate (gen_smiles):\n",
    "    # make sure all the SMILES are Canonical\n",
    "    ss = Chem.MolToSmiles(Chem.MolFromSmiles(s, sanitize=True), canonical=True)\n",
    "    gen_smiles[i] = ss\n",
    "\n",
    "output['SMILES'] = gen_smiles\n",
    "output['des_gap'] = sample_ys\n",
    "# More accurate for regressor to predict gap from canonical SMILES\n",
    "output['pred_gap'] = predss_can\n",
    "output['Err_pred_des'] = [abs(i- j)/i for i, j in zip(output['des_gap'], output['pred_gap'])]\n",
    "output = pd.DataFrame(output)\n",
    "output.reset_index(drop=True, inplace=True)\n",
    "output.to_csv ('./../experiments/regular/Initial_training.csv', index=False)\n",
    "\n",
    "## Statistics  (# pred=True value, Des=prediction)\n",
    "N = len(predss_can)\n",
    "# Explained Variance R2 from sklearn.metrics.explained_variance_score\n",
    "explained_variance_R2_pred_des = explained_variance_score(output['des_gap'], output['pred_gap'])\n",
    "print (\"explained_varice_R2_pred_des\", explained_variance_R2_pred_des)\n",
    "rsquared = np.round (r2_score (output['des_gap'], output['pred_gap']), 4)\n",
    "print (\"r squared r**2\", rsquared)\n",
    "\n",
    "# mean absolute error \n",
    "MAE_pred_des = np.round (mean_absolute_error(output['pred_gap'], output['des_gap']), 4)\n",
    "print (\"MAE_pred_des\", MAE_pred_des)\n",
    "# Fractioned MAE, more normalized\n",
    "Fractioned_MAE_pred_des = 0\n",
    "for pred, des in zip(output['pred_gap'], output['des_gap']):\n",
    "    Fractioned_MAE_pred_des = Fractioned_MAE_pred_des +  abs(des-pred)/des\n",
    "Fractioned_MAE_pred_des = Fractioned_MAE_pred_des/N\n",
    "\n",
    "# root mean squared error (RMSE), sqrt(sklearn ouputs MSE)\n",
    "RMSE_pred_des = mean_squared_error(output['pred_gap'], output['des_gap'])**0.5\n",
    "\n",
    "Fractioned_RMSE_pred_des = 0\n",
    "for pred, des in zip(output['pred_gap'], output['des_gap']):\n",
    "    Fractioned_RMSE_pred_des = Fractioned_RMSE_pred_des + ((des-pred)/des)**2\n",
    "Fractioned_RMSE_pred_des = (Fractioned_RMSE_pred_des/N)**0.5\n",
    "\n",
    "output2 = output.drop_duplicates(['SMILES'])\n",
    "output2.reset_index(drop = True, inplace = True)\n",
    "output2.to_csv('./../experiments/regular/Initial_training_nodub.csv', index = False)\n",
    "\n",
    "less20RE_per = np.round ((sum(output['Err_pred_des'] <= 0.2) / output['Err_pred_des'].shape[0]), 4)\n",
    "print ('% < 20 RE', less20RE_per)\n",
    "output_len = len(output)\n",
    "explained_variance_R2_pred_des = explained_variance_score(output['des_gap'], output['pred_gap'])\n",
    "#print (\"explained_varice_R2_pred_des\", explained_variance_R2_pred_des)\n",
    "mean_RE = np.round (np.mean (output['Err_pred_des']), 4)\n",
    "print ('RE mean', mean_RE)\n",
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
